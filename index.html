<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Translation System</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f4f4f4;
        }
        header {
            background: linear-gradient(to right, #86c240, #ffcb0a);
            color: #ffffff;
            padding: 10px 0;
            text-align: center;
        }
        .navbar {
            background-color: #333;
            overflow: hidden;
            display: flex;
            justify-content: center; 
        }
        .navbar a {
            float: none; 
            display: block;
            color: white;
            text-align: center;
            padding: 14px 16px;
            text-decoration: none;
        }
        .navbar a:hover {
            background-color: #ddd;
            color: black;
        }
        section {
            background: #ffffff;
            border: 1px solid #dddddd;
            border-radius: 5px;
            margin: 20px 0;
            padding: 15px;
            display: none; /* Hide all sections by default */
        }
        section.active {
            display: block; /* Show only the active section */
        }
        h2 {
            color: #35424a;
        }
    </style>
</head>

<body>
    <header>
        <h1>Voice Translation System</h1>
    </header>

    <div class="navbar">
        <a href="#" onclick="showContent('overview')">Overview</a>
        <a href="#" onclick="showContent('proposal')">Proposal</a>
        <a href="#" onclick="showContent('midterm-checkpoint')">Midterm Checkpoint</a>
        <a href="#" onclick="showContent('final-report')">Final Report</a>
    </div>

    <section id="overview" class="content active">
        <h2>Overview</h2>
        <p>
            The Voice Translation System aims to bridge communication gaps by providing accurate translations of spoken language. In today’s interconnected world, voice translation systems have become essential tools for effective communication across diverse languages. Innovations from tech giants like Google with Google Translate and Meta's new Ray-Ban smart glasses highlight the growing importance of voice translation technology, making it more accessible and practical in everyday situations.
        </p>
        <p>
            Our project aspires to design an effective and accurate translator that not only competes with these established solutions but also addresses their limitations. By leveraging advanced machine learning techniques, we will develop a robust translation system that can enhance the quality and speed of translations, making them more reliable for users in real-time communication.
        </p>
        <ul>
            <li>Develop a robust translation system leveraging machine learning techniques.</li>
            <li>Utilize diverse datasets for training to improve translation accuracy.</li>
            <li>Implement user-friendly interfaces for easy interaction.</li>
            <li>Ensure system scalability for multiple languages and dialects.</li>
            <li>Focus on minimizing latency to facilitate real-time voice translations, catering to users in dynamic environments.</li>
            <li>Incorporate feedback mechanisms to continuously enhance translation quality based on user interactions.</li>
        </ul>
    </section>
    
    
    <section id="proposal" class="content">
        <h2>Proposal</h2>
        <p><strong>Introduction/Background:</strong></p>
        <p>
            At the core of many modern voice translation systems is the application of advanced machine learning techniques, notably Long Short-Term Memory (LSTM) networks. Originally, Google Translate relied heavily on LSTMs as part of its neural machine translation (NMT) framework, specifically through the Google Neural Machine Translation (GNMT) model introduced in 2016 [2]. 
        </p>
        <p>
            Text-to-text translation was made possible by the development of the Transformer architecture. The Transformer model eliminated the need for RNNs and instead relied solely on self-attention mechanisms and positional encoding to capture relationships between words in a sequence.
        </p>
        <p>
            The <a href="https://tatoeba.org/en/downloads">Tatoeba English-Spanish Dataset</a> contains over 265,817 sentence pairs, supporting multilingual NLP tasks, including machine translation, and facilitating linguistic research and model training. The English-Spanish Dataset consists of pairs of sentences in English (source language) and their corresponding translations in Spanish (target language), providing a level of linguistic variety and flexibility.
        </p>
    
        <p><strong>Problem Definition:</strong></p>
        <p>
            The problem we’re aiming to improve is the need for more accurate and efficient voice translations for individuals traveling or engaging in communication with people who speak different languages.
        </p>
    
        <p><strong>Methods:</strong></p>
        <p><strong>Data Preprocessing Methods Identified:</strong></p>
        <ul>
            <li>
                <strong>Data Cleaning:</strong> Lowercasing all sentences, removing punctuation, eliminating duplicate sentence pairs, and rows with missing translations.
            </li>
            <li>
                <strong>BERT:</strong> Utilizing this model allows for richer feature extraction, leading to improved translation accuracy. BERT's pre-trained language representations can be fine-tuned for specific tasks, including translation.
            </li>
            <li>
                <strong>Contraction Integration:</strong> Expanding contractions (e.g., "don't," "isn't") to their full forms (e.g., "do not," "is not") during preprocessing, and creating a new duplicate dataset where contractions are present to improve overall translation.
            </li>
        </ul>
    
        <p><strong>ML Algorithms/Models Identified:</strong></p>
        <ul>
            <li>
                <strong>GRU (Gated Recurrent Unit):</strong> Combines input and forget gates into a single update gate, allowing them to efficiently capture dependencies in sequential data, making them suitable for tasks like machine translation.
            </li>
            <li>
                <strong>LSTM (Long Short-Term Memory):</strong> A type of recurrent neural network that utilizes a complex gating mechanism to maintain context over long sequences, effectively managing the flow of information for accurate language translation.
            </li>
            <li>
                <strong>Transformers:</strong> Leverage self-attention mechanisms to process input sequences in parallel, significantly improving training efficiency and translation accuracy compared to traditional RNN-based models.
            </li>
        </ul>
    
        <p><strong>(Potential) Results and Discussion:</strong></p>
        <p><strong>Quantitative Metrics:</strong></p>
        <ul>
            <li>
                <strong>BLEU:</strong> A quantitative metric used to evaluate the quality of machine translation output, measuring how many words and phrases from the generated translation match reference translations. The score ranges from 0 to 1, with higher scores indicating better translation quality.
            </li>
            <li>
                <strong>F1 Score:</strong> Combines precision and recall, providing a balanced measure of a model's accuracy, particularly useful for imbalanced datasets.
            </li>
            <li>
                <strong>Loss:</strong> Measures the difference between the predicted output of the model and the actual output during training. Lower loss indicates good performance, while higher loss suggests the need for improvement.
            </li>
            <li>
                <strong>Overfitting:</strong> Occurs when a model learns the training data too well, capturing noise instead of general patterns. Evaluation metrics for overfitting assess performance on unseen data versus training performance.
            </li>
            <li>
                <strong>Token Differences and Similarities:</strong> Analyzing generated translations by comparing individual tokens (words or subwords) to see how they differ from reference translations.
            </li>
        </ul>
    
        <p><strong>Project Goals:</strong></p>
        <ul>
            <li>
                <strong>Improve Translation Accuracy:</strong> Achieve high-quality translations across multiple languages, measured by BLEU or TER scores.
            </li>
            <li>
                <strong>Latency:</strong> Minimize the time taken for the translation process to ensure real-time translations for applications like voice translation.
            </li>
        </ul>
    
        <p><strong>Expected Results:</strong></p>
        <ul>
            <li>A fully working Voice Translation System from English to Spanish.</li>
            <li>A trained text-to-text translation model with quantifiable improvements over baseline models.</li>
            <li>Measurable improvements in translation quality using BLEU/TER scores compared to off-the-shelf translation solutions.</li>
        </ul>
    
        <p><strong>References:</strong></p>
        <ol>
            <li>
                M. H. A. R. Al-Azzeh and H. A. A. Al-Ramahi, "Voice Translation System: A Review," <em>International Journal of Advanced Computer Science and Applications</em>, vol. 10, no. 1, pp. 265-272, 2019. DOI: 10.14569/IJACSA.2019.0100133. <a href="#">Link</a>.
            </li>
            <li>
                Wu, Y., et al. "Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation." Google Research, 2016. <a href="#">Link</a>.
            </li>
            <li>
                M. G. Zeyer, J. G. von Neumann, and A. J. Spang, "Evaluating the Effectiveness of Voice Translation Systems for Communication in International Business," <em>Journal of Language and Business</em>, vol. 9, no. 2, pp. 1-15, 2020. <a href="#">Link</a>.
            </li>
            <li>
                Bahdanau, D., Cho, K., and Bengio, Y. "Neural Machine Translation by Jointly Learning to Align and Translate." ICLR, 2015. <a href="#">Link</a>.
            </li>
            <li>
                "Model Behind Google Translate: Seq2seq in Machine Learning." Analytics Vidhya, Feb. 2023. <a href="#">Link</a>.
            </li>
        </ol>

        <p><strong>Gantt Chart | Contribution Table</strong></p>

        <iframe src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQ859xpT-fweSU_K_c8evd4WJc1w4r-YjaK_QBihn6EFeHcOKbhbD3Qm7nHtzihMFuYEx0O0iS-vfqk/pubhtml?gid=610421560&amp;single=true&amp;widget=true&amp;headers=false" width="100%" height="400"></iframe>

        <p><strong>Video Presentation</strong></p>

        <iframe width="560" height="315" src="https://www.youtube.com/embed/NKKxuRSPiWY?si=BHstRo7EnTZM6IoP" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        
        <p>Voice Translation System : <a href="https://github.com/MosesTheRedSea/Voice-Based-Text-Language-Translation-System"> GitHub Repository</a></p>

    </section>
    

    <section id="midterm-checkpoint" class="content">
        <h2>Midterm Checkpoint</h2>
        <p>
            Here is our midterm checkpoint for our Voice-Based Language Translation System, focusing on data preprocessing, machine learning, and model training for English-Spanish translation using a Sequence-to-Sequence (Seq2Seq) model with a GRU-based encoder-decoder architecture.
        </p>

        <h3>Introduction/Background</h3>
        <p>
            The Voice Translation System aims to bridge communication gaps by providing accurate translations of spoken language. In today’s interconnected world, voice translation systems have become essential tools for effective communication across diverse languages. Innovations from tech giants like Google with Google Translate and Meta's new Ray-Ban smart glasses highlight the growing importance of voice translation technology, making it more accessible and practical in everyday situations.
        </p>
        <p>
            Our project aspires to design an effective and accurate translator that not only competes with these established solutions but also addresses their limitations. By leveraging advanced machine learning techniques, we will develop a robust translation system that can enhance the quality and speed of translations, making them more reliable for users in real-time communication.
        </p>
        
        <h3>Problem Definition</h3>
        <p>
            The problem we’re aiming to improve is the need for more accurate and efficient voice translations for individuals traveling or engaging in communication with people who speak different languages.
        </p>
    
    
        <h3>Methods</h3>
        <p>
            The preprocessing of the dataset is performed using various techniques:
        </p>
        <ul>
            <li><strong>Lowercasing:</strong> All text is converted to lowercase to maintain consistency.</li>
            <li><strong>Punctuation Removal:</strong> Both English and Spanish sentences have their punctuation removed to make translation easier.</li>
            <li><strong>Removing Duplicates:</strong> Duplicate sentence pairs are dropped to avoid redundancy in the training data.</li>
            <li><strong>Handling Contractions:</strong> A contraction dictionary is applied to expand contractions in the English text, improving model accuracy by reducing variation in language forms.</li>
        </ul>
    
        <ul>
            <li><strong>dataSetCleaning(df):</strong> This function performs lowercasing, punctuation removal, and duplicate elimination.
                <pre><code class="language-python">
        def dataSetCleaning(df):
            # Lowercasing all sentences
            df['English'] = df['English'].str.lower()
            df['Spanish'] = df['Spanish'].str.lower().fillna('')
        
            # Removing Punctuation From Both Data set's so that translation will be easier
            df['English'] = df['English'].str.translate(str.maketrans('', '', string.punctuation))
            df['Spanish'] = df['Spanish'].str.translate(str.maketrans('', '', string.punctuation))
        
            # Eliminating duplicate sentence pairs
            df.drop_duplicates(subset=['English', 'Spanish'])
        
            #  Remove rows with missing translations.
            df[df['Spanish'] != '']
            return df
                </code></pre>
            </li>

            <p> In order to fully expand the contraction we had a python file which held a dictionary of the major contractions and their expanded form in english</p>
           
            <pre><code>
                CONTRACTIONS = {
                    "i'm": "I am",
                    "you're": "you are",
                    "he's": "he is",
                    "she's": "she is",
                    "it's": "it is",
                    "we're": "we are",
                    "they're": "they are",
                    "i've": "I have",
                    "you've": "you have",
                    "we've": "we have",
                    "they've": "they have",
                    "i'd": "I would",
                    "you'd": "you would",
                    "he'd": "he would",
                    "she'd": "she would",
                    "we'd": "we would",
                    "they'd": "they would",
                    "i'll": "I will",
                    "you'll": "you will",
            </code></pre>

            <li><strong>dataSetContractionIntegration(df):</strong> Expands contractions in the English sentences using a predefined contraction dictionary.
                <pre><code class="language-python">
        def dataSetContractionIntegration(df):
            new_data = []
            for _, row in df.iterrows():
                words = row["English"].split()
                expanded_words = [CONTRACTIONS[word.lower()] if word.lower() in CONTRACTIONS else word for word in words]
                expanded_sentence = ' '.join(expanded_words)
                english_sentence = {
                    "English": expanded_sentence,
                    "Spanish": row["Spanish"]
                }
                new_data.append(english_sentence)
            dataFrame = pd.DataFrame(new_data)
            return pd.concat([df, dataFrame]).drop_duplicates().reset_index(drop=True)
                </code></pre>
            </li>
        
            <li><strong>dataSetBertEmbeddings(text, model, tokenizer):</strong> Utilizes the BERT tokenizer and model from Hugging Face to obtain word embeddings for tokenized text.</li>

            <pre><code>
                def dataSetBertEmbeddings(text, model, tokenizer):
                    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
                    with torch.no_grad():
                        output = model(**encoded_input)
                    word_embeddings = output.last_hidden_state[:, 0, :]
                    return word_embeddings.squeeze().numpy()  

            </code></pre>

            <p> In order to get the BERT Embeedings for the English & Spanish Sentences we used a transformer model through the Huggingface API</p>

            <pre><code>
                english_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
                english_model = BertModel.from_pretrained("bert-base-uncased")

                # We Found BETO : A Spanish BERT (Tokenization)
                # https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased
                spanish_tokenizer = BertTokenizer.from_pretrained("dccuchile/bert-base-spanish-wwm-cased")
                spanish_model = BertModel.from_pretrained("dccuchile/bert-base-spanish-wwm-cased")

                # Get BERT embeddings for English
                df['English BERT'] = df['English'].apply(lambda x: dataSetBertEmbeddings(x, english_model, english_tokenizer))

                # Get BERT embeddings for Spanish
                df['Spanish BERT'] = df['Spanish'].apply(lambda x: dataSetBertEmbeddings(x, spanish_model, spanish_tokenizer))

            </code></pre>
    
            <p>
                The translation system utilizes a custom Seq2Seq model with a GRU-based Encoder and Decoder. This architecture is commonly used for machine translation tasks due to its ability to capture long-range dependencies and generate output sequences.
            </p>
        
            <h4>Key Components:</h4>
            <ul>
                <li><strong>Encoder:</strong> Encodes the input (English) sentence into a context vector.</li>
                <pre><code>
                    class Encoder(nn.Module):
                        def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):
                            super(Encoder, self).__init__()
                            self.embedding = nn.Embedding(input_dim, embedding_dim)  # Using embedding_dim
                            self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=dropout)
                            self.dropout = nn.Dropout(dropout)

                        def forward(self, input):
                            embedded = self.embedding(input)
                            embedded = self.dropout(embedded)
                            outputs, hidden = self.gru(embedded)
                            return outputs, hidden
                </code></pre>
                <li><strong>Decoder:</strong> Decodes the context vector into the output (Spanish) sentence.</li>
                <pre><code>
                    class Decoder(nn.Module):
                        def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):
                            super(Decoder, self).__init__()
                            self.embedding = nn.Embedding(output_dim, embedding_dim)
                            self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=dropout)
                            self.fc_out = nn.Linear(hidden_dim, output_dim)
                            self.dropout = nn.Dropout(dropout)

                        def forward(self, input, hidden):
                            # Convert input token indices to embeddings
                            embedded = self.embedding(input).unsqueeze(0)  # (1, batch_size, embedding_dim)
                            embedded = self.dropout(embedded)

                            # Adjust hidden dimensions for single-batch case
                            if hidden.dim() == 3 and hidden.size(1) == 1:
                                hidden = hidden.squeeze(1)  # Squeeze batch dimension for unbatched case

                            # Pass through GRU
                            output, hidden = self.gru(embedded, hidden.unsqueeze(1) if hidden.dim() == 2 else hidden)

                            # Generate predictions
                            prediction = self.fc_out(output.squeeze(0))
                            return prediction, hidden.squeeze(1) if hidden.dim() == 3 else hidden
                </code></pre>
                <li><strong>Seq2Seq Model:</strong> This model ties together the encoder and decoder, making it a complete sequence-to-sequence framework for translation.</li>

                <pre><code>
                    class Seq2Seq(nn.Module):
                        def __init__(self, encoder, decoder, device):
                            super().__init__()
                            self.encoder = encoder
                            self.decoder = decoder
                            self.device = device

                        def forward(self, src, trg, teacher_forcing_ratio=0.5):
                            batch_size = src.shape[1]
                            trg_len = trg.shape[0]
                            trg_vocab_size = self.decoder.fc_out.out_features

                            outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
                            _, hidden = self.encoder(src)  # Ensure only hidden is passed

                            input = trg[0, :]  # First target token
                            for t in range(1, trg_len):
                                output, hidden = self.decoder(input, hidden)
                                outputs[t] = output

                                top1 = output.argmax(1)
                                input = trg[t] if torch.rand(1).item() < teacher_forcing_ratio else top1

                            return outputs
                </code></pre>
        </ul>
    
        <h4>Implementation Details:</h4>
        <ul>
            <li>The TranslationDataset class prepares the dataset, including tokenizing sentences and converting them into tensors for model training.</li>
            <li>The model uses Cross-Entropy Loss for optimization, suitable for classification tasks such as predicting each token in the target sequence.</li>
            <li>BLEU and F1 scores are computed during training as evaluation metrics for translation quality.</li>
        </ul>

        <h3>Results and Discussion</h3>

        <p>
            The training loop runs for 10 epochs, where:
        </p>
        <ul>
            <li>The model is trained with batch-wise data using the DataLoader object.</li>
            <li>At each epoch, the model computes the loss, BLEU score, and F1 score for the translation quality.</li>
            <li>A checkpoint is saved after each epoch to allow model recovery if needed.</li>
        </ul><br>

        <div style="text-align: center;">
            <img src="metrics_per_epoch.png" alt="Training Results" width="1200" />
            <p>Figure 1: Plot of Training Loss, BLEU, and F1 Scores over Epochs</p>
        </div>

        <ul>

            <p>
            <li><strong>Loss</strong> Loss quantifies how far off the model's predictions are from the expected results. </li>
            According to the Graph above, The training loss is decreasing steadily across epochs, indicating that the model is learning and improving. This is a good sign.
            However, the rate of decrease does slow down a bit It's possible that the model has reached a point of diminishing returns, where further training might not significantly improve the loss.
            </p>

            <p>
            <li><strong>BLEU score:</strong> Measures the quality of the generated translations by comparing them to reference translations.</li>
            The BLEU score decreases steadily across all epochs. This suggests that the model's translation quality should be increasing. It starts at 0.03284 and ends at
            0.03279. According to BLEU,  0.7 - 0.9: Good translation quality and 0.9 - 1.0: Excellent translation quality.
            </p>
            
            <p>
                <li><strong>F1 Score:</strong> A metric for classification performance, particularly useful for evaluating precision and recall in multi-class tasks.</li>
                Our F1 score increased at the beginner reaching a value of 0.006555 it then dips downards to 0.006530. However as we reach the 5th Epoch it increases back to
                0.006545. Our F1 score is increasing which means our model's perceision is increasing. According to F1, 0.7 - 0.9: Good performance and 0.9 - 1.0: Excellent performance
            </p>
            
            <strong> Next Steps </strong>
            <li>BLEU and F1 scores over epochs, showing how well the model's translation quality improves during training.</li>
            <p>
                To improve our model's performance, we will begin by closely monitoring both training and validation losses to identify any overfitting or underfitting issues. 
                We will adjust the learning rate and experiment with different batch sizes to stabilize training. 
                Additionally, we will review our data preprocessing pipeline for any inconsistencies and ensure the quality of our translations. 
                Exploring more advanced architectures, such as the Transformer or models with attention mechanisms, could enhance translation quality. Finally, implementing early stopping and tuning hyperparameters will help us optimize the model and address the decreasing BLEU score.
            </p>
            
        </ul>
    
        <p><strong>References:</strong></p>
        <ol>
            <li>
                M. H. A. R. Al-Azzeh and H. A. A. Al-Ramahi, "Voice Translation System: A Review," <em>International Journal of Advanced Computer Science and Applications</em>, vol. 10, no. 1, pp. 265-272, 2019. DOI: 10.14569/IJACSA.2019.0100133. <a href="#">Link</a>.
            </li>
            <li>
                Wu, Y., et al. "Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation." Google Research, 2016. <a href="#">Link</a>.
            </li>
            <li>
                M. G. Zeyer, J. G. von Neumann, and A. J. Spang, "Evaluating the Effectiveness of Voice Translation Systems for Communication in International Business," <em>Journal of Language and Business</em>, vol. 9, no. 2, pp. 1-15, 2020. <a href="#">Link</a>.
            </li>
            <li>
                Bahdanau, D., Cho, K., and Bengio, Y. "Neural Machine Translation by Jointly Learning to Align and Translate." ICLR, 2015. <a href="#">Link</a>.
            </li>
            <li>
                "Model Behind Google Translate: Seq2seq in Machine Learning." Analytics Vidhya, Feb. 2023. <a href="#">Link</a>.
            </li>
        </ol>
        
        <p><strong>Contribution Table</strong></p>

        <table style="width: 100%; border-collapse: collapse;">
            <thead>
                <tr>
                    <th style="border: 1px solid black; padding: 8px; text-align: left;">Team Member</th>
                    <th style="border: 1px solid black; padding: 8px; text-align: left;">Midterm Contributions</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td style="border: 1px solid black; padding: 8px;">Moses Adewolu</td>
                    <td style="border: 1px solid black; padding: 8px;">Implemented Preprocessing methods, data cleaning, contraction integration, BERT Embedding via HuggingFace API.
                     Implemented GRU Model, Encoder, Decoder along with SequenceToSequence Model. Worked on Method training code along with method evaluation and results. Worked on miterm proprosal. </td>
                </tr>
                <tr>
                    <td style="border: 1px solid black; padding: 8px;">Christian</td>
                    <td style="border: 1px solid black; padding: 8px;">Helped find dataset, worked on preprocessing methods, dataSetContractionIntegration, dataCleaning. Helped with training the model
                        on PACE ICE.
                    </td>
                </tr>
                <tr>
                    <td style="border: 1px solid black; padding: 8px;">Ethan</td>
                    <td style="border: 1px solid black; padding: 8px;">Helped work on midterm proposal presentation. Formatted Results, specifically BLEU, Loss, F1 Scores.</td>
                </tr>
                <tr>
                    <td style="border: 1px solid black; padding: 8px;">Arun</td>
                    <td style="border: 1px solid black; padding: 8px;">Helped work on midterm proposal presentation. Formatted Results, specifically BLEU, Loss, F1 Scores.</td>
                </tr>
                <!-- Add more rows as necessary -->
            </tbody>
        </table>

        <p><strong>Gantt Chart </strong></p>

        <ul>
            <li>
                <iframe src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQ859xpT-fweSU_K_c8evd4WJc1w4r-YjaK_QBihn6EFeHcOKbhbD3Qm7nHtzihMFuYEx0O0iS-vfqk/pubhtml?gid=610421560&amp;single=true&amp;widget=true&amp;headers=false" 
                        width="100%" height="400" frameborder="0" allowfullscreen></iframe>
            </li>
        </ul>
        

    </section>
    

    <section id="final-report" class="content">
        <h2>Final Report</h2>
        <p>
            This section will include your final report, summarizing the work completed, results obtained, and conclusions drawn from the project.
        </p>
    </section>

    <footer>
        <p>&copy; 2024 MosesTheRedSea. All rights reserved.</p>
    </footer>

    <script>
        function showContent(sectionId) {
            // Hide all content sections
            var contents = document.getElementsByClassName('content');
            for (var i = 0; i < contents.length; i++) {
                contents[i].style.display = 'none'; // Hide all sections
            }

            // Show the selected content section
            document.getElementById(sectionId).style.display = 'block'; // Show the selected section
        }

        // Show the overview section by default when the page loads
        document.addEventListener('DOMContentLoaded', function() {
            showContent('overview'); // You can change 'overview' to 'proposal' to show it initially
        });
    </script>

</body>
</html>
