<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Translation System</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f4f4f4;
        }
        header {
            background: linear-gradient(to right, #0547a2, #0077cc);
            color: #ffffff;
            padding: 10px 0;
            text-align: center;
        }
        .navbar {
            background-color: #333;
            overflow: hidden;
            display: flex;
            justify-content: center; 
        }
        .navbar a {
            float: none; 
            display: block;
            color: white;
            text-align: center;
            padding: 14px 16px;
            text-decoration: none;
        }
        .navbar a:hover {
            background-color: #ddd;
            color: black;
        }
        section {
            background: #ffffff;
            border: 1px solid #dddddd;
            border-radius: 5px;
            margin: 20px 0;
            padding: 15px;
            display: none; /* Hide all sections by default */
        }
        section.active {
            display: block; /* Show only the active section */
        }
        h2 {
            color: #35424a;
        }
    </style>
</head>

<body>
    <header>
        <h1>Voice Translation System</h1>
    </header>

    <div class="navbar">
        <a href="#" onclick="showContent('overview')">Overview</a>
        <a href="#" onclick="showContent('proposal')">Proposal</a>
        <a href="#" onclick="showContent('midterm-checkpoint')">Midterm Checkpoint</a>
        <a href="#" onclick="showContent('final-report')">Final Report</a>
    </div>

    <section id="overview" class="content active">
        <h2>Overview</h2>
        <p>
            The Voice Translation System aims to bridge communication gaps by providing accurate translations of spoken language. In today’s interconnected world, voice translation systems have become essential tools for effective communication across diverse languages. Innovations from tech giants like Google with Google Translate and Meta's new Ray-Ban smart glasses highlight the growing importance of voice translation technology, making it more accessible and practical in everyday situations.
        </p>
        <p>
            Our project aspires to design an effective and accurate translator that not only competes with these established solutions but also addresses their limitations. By leveraging advanced machine learning techniques, we will develop a robust translation system that can enhance the quality and speed of translations, making them more reliable for users in real-time communication.
        </p>
        <ul>
            <li>Develop a robust translation system leveraging machine learning techniques.</li>
            <li>Utilize diverse datasets for training to improve translation accuracy.</li>
            <li>Implement user-friendly interfaces for easy interaction.</li>
            <li>Ensure system scalability for multiple languages and dialects.</li>
            <li>Focus on minimizing latency to facilitate real-time voice translations, catering to users in dynamic environments.</li>
            <li>Incorporate feedback mechanisms to continuously enhance translation quality based on user interactions.</li>
        </ul>
    </section>
    
    
    <section id="proposal" class="content">
        <h2>Proposal</h2>
        <p><strong>Introduction/Background:</strong></p>
        <p>
            At the core of many modern voice translation systems is the application of advanced machine learning techniques, notably Long Short-Term Memory (LSTM) networks. Originally, Google Translate relied heavily on LSTMs as part of its neural machine translation (NMT) framework, specifically through the Google Neural Machine Translation (GNMT) model introduced in 2016 [2]. 
        </p>
        <p>
            Text-to-text translation was made possible by the development of the Transformer architecture. The Transformer model eliminated the need for RNNs and instead relied solely on self-attention mechanisms and positional encoding to capture relationships between words in a sequence.
        </p>
        <p>
            The <a href="https://tatoeba.org/en/downloads">Tatoeba English-Spanish Dataset</a> contains over 265,817 sentence pairs, supporting multilingual NLP tasks, including machine translation, and facilitating linguistic research and model training. The English-Spanish Dataset consists of pairs of sentences in English (source language) and their corresponding translations in Spanish (target language), providing a level of linguistic variety and flexibility.
        </p>
    
        <p><strong>Problem Definition:</strong></p>
        <p>
            The problem we’re aiming to improve is the need for more accurate and efficient voice translations for individuals traveling or engaging in communication with people who speak different languages.
        </p>
    
        <p><strong>Methods:</strong></p>
        <p><strong>Data Preprocessing Methods Identified:</strong></p>
        <ul>
            <li>
                <strong>Data Cleaning:</strong> Lowercasing all sentences, removing punctuation, eliminating duplicate sentence pairs, and rows with missing translations.
            </li>
            <li>
                <strong>BERT:</strong> Utilizing this model allows for richer feature extraction, leading to improved translation accuracy. BERT's pre-trained language representations can be fine-tuned for specific tasks, including translation.
            </li>
            <li>
                <strong>Contraction Integration:</strong> Expanding contractions (e.g., "don't," "isn't") to their full forms (e.g., "do not," "is not") during preprocessing, and creating a new duplicate dataset where contractions are present to improve overall translation.
            </li>
        </ul>
    
        <p><strong>ML Algorithms/Models Identified:</strong></p>
        <ul>
            <li>
                <strong>GRU (Gated Recurrent Unit):</strong> Combines input and forget gates into a single update gate, allowing them to efficiently capture dependencies in sequential data, making them suitable for tasks like machine translation.
            </li>
            <li>
                <strong>LSTM (Long Short-Term Memory):</strong> A type of recurrent neural network that utilizes a complex gating mechanism to maintain context over long sequences, effectively managing the flow of information for accurate language translation.
            </li>
            <li>
                <strong>Transformers:</strong> Leverage self-attention mechanisms to process input sequences in parallel, significantly improving training efficiency and translation accuracy compared to traditional RNN-based models.
            </li>
        </ul>
    
        <p><strong>(Potential) Results and Discussion:</strong></p>
        <p><strong>Quantitative Metrics:</strong></p>
        <ul>
            <li>
                <strong>BLEU:</strong> A quantitative metric used to evaluate the quality of machine translation output, measuring how many words and phrases from the generated translation match reference translations. The score ranges from 0 to 1, with higher scores indicating better translation quality.
            </li>
            <li>
                <strong>F1 Score:</strong> Combines precision and recall, providing a balanced measure of a model's accuracy, particularly useful for imbalanced datasets.
            </li>
            <li>
                <strong>Loss:</strong> Measures the difference between the predicted output of the model and the actual output during training. Lower loss indicates good performance, while higher loss suggests the need for improvement.
            </li>
            <li>
                <strong>Overfitting:</strong> Occurs when a model learns the training data too well, capturing noise instead of general patterns. Evaluation metrics for overfitting assess performance on unseen data versus training performance.
            </li>
            <li>
                <strong>Token Differences and Similarities:</strong> Analyzing generated translations by comparing individual tokens (words or subwords) to see how they differ from reference translations.
            </li>
        </ul>
    
        <p><strong>Project Goals:</strong></p>
        <ul>
            <li>
                <strong>Improve Translation Accuracy:</strong> Achieve high-quality translations across multiple languages, measured by BLEU or TER scores.
            </li>
            <li>
                <strong>Latency:</strong> Minimize the time taken for the translation process to ensure real-time translations for applications like voice translation.
            </li>
        </ul>
    
        <p><strong>Expected Results:</strong></p>
        <ul>
            <li>A fully working Voice Translation System from English to Spanish.</li>
            <li>A trained text-to-text translation model with quantifiable improvements over baseline models.</li>
            <li>Measurable improvements in translation quality using BLEU/TER scores compared to off-the-shelf translation solutions.</li>
        </ul>
    
        <p><strong>References:</strong></p>
        <ol>
            <li>
                M. H. A. R. Al-Azzeh and H. A. A. Al-Ramahi, "Voice Translation System: A Review," <em>International Journal of Advanced Computer Science and Applications</em>, vol. 10, no. 1, pp. 265-272, 2019. DOI: 10.14569/IJACSA.2019.0100133. <a href="#">Link</a>.
            </li>
            <li>
                Wu, Y., et al. "Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation." Google Research, 2016. <a href="#">Link</a>.
            </li>
            <li>
                M. G. Zeyer, J. G. von Neumann, and A. J. Spang, "Evaluating the Effectiveness of Voice Translation Systems for Communication in International Business," <em>Journal of Language and Business</em>, vol. 9, no. 2, pp. 1-15, 2020. <a href="#">Link</a>.
            </li>
            <li>
                Bahdanau, D., Cho, K., and Bengio, Y. "Neural Machine Translation by Jointly Learning to Align and Translate." ICLR, 2015. <a href="#">Link</a>.
            </li>
            <li>
                "Model Behind Google Translate: Seq2seq in Machine Learning." Analytics Vidhya, Feb. 2023. <a href="#">Link</a>.
            </li>
        </ol>

        <p><strong>Gantt Chart | Contribution Table</strong></p>

        <iframe src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQ859xpT-fweSU_K_c8evd4WJc1w4r-YjaK_QBihn6EFeHcOKbhbD3Qm7nHtzihMFuYEx0O0iS-vfqk/pubhtml?gid=610421560&amp;single=true&amp;widget=true&amp;headers=false" width="100%" height="400"></iframe>

        <p><strong>Video Presentation</strong></p>

        <iframe width="560" height="315" src="https://www.youtube.com/embed/NKKxuRSPiWY?si=BHstRo7EnTZM6IoP" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        
        <p>Voice Translation System : <a href="https://github.com/MosesTheRedSea/Voice-Based-Text-Language-Translation-System"> GitHub Repository</a></p>

    </section>
    

    <section id="midterm-checkpoint" class="content">
        <h2>Midterm Checkpoint</h2>
        <p>
            This section will contain your midterm checkpoint details, including any updates on your progress, challenges faced, and changes to the proposal as necessary.
        </p>
    </section>

    <section id="final-report" class="content">
        <h2>Final Report</h2>
        <p>
            This section will include your final report, summarizing the work completed, results obtained, and conclusions drawn from the project.
        </p>
    </section>

    <footer>
        <p>&copy; 2024 MosesTheRedSea. All rights reserved.</p>
    </footer>

    <script>
        function showContent(sectionId) {
            // Hide all content sections
            var contents = document.getElementsByClassName('content');
            for (var i = 0; i < contents.length; i++) {
                contents[i].style.display = 'none'; // Hide all sections
            }

            // Show the selected content section
            document.getElementById(sectionId).style.display = 'block'; // Show the selected section
        }

        // Show the overview section by default when the page loads
        document.addEventListener('DOMContentLoaded', function() {
            showContent('overview'); // You can change 'overview' to 'proposal' to show it initially
        });
    </script>

</body>
</html>
